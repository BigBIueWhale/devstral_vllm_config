# vLLM v0.15.1 server config for Mistral Devstral Small 2 24B (AWQ 4-bit) on NVIDIA GeForce RTX 5090
# This file is loaded by the Docker entrypoint via: --config /workspace/config.yaml

# ---- Model ----
# Community AWQ 4-bit quantization of mistralai/Devstral-Small-2-24B-Instruct-2512.
# vLLM auto-detects the compressed-tensors quantization format from config.json.
model: "cyankiwi/Devstral-Small-2-24B-Instruct-2512-AWQ-4bit"
revision: "da6366ed3bb6d5207c6544ede10df31e4082e027"
host: "0.0.0.0"
port: 8000

# ---- Config and weight format (workaround for vLLM v0.15.1 bug) ----
# Two bugs in vLLM v0.15.1 prevent loading this AWQ model with default settings:
#
# 1. Mistral config path (config-format auto): params.json contains a vision_encoder
#    section, causing vLLM's Mistral config adapter to route the model through
#    PixtralForConditionalGeneration instead of the correct Mistral3ForConditionalGeneration.
#    This crashes with KeyError: 'merging_layer.weight' in pixtral.py.
#
# 2. HuggingFace config path (config-format "hf"): config.json declares
#    text_config.model_type "ministral3", which transformers 4.57.6 (bundled in
#    the vLLM v0.15.1 Docker image) does not recognize — KeyError: 'ministral3'.
#
# Workaround: config_override.json is a copy of the AWQ model's config.json with
# text_config.model_type patched from "ministral3" to "mistral" (compatible with
# transformers 4.57.6). The top-level architectures field is unchanged
# ("Mistral3ForConditionalGeneration"), so vLLM loads the correct model class.
# hf-config-path points vLLM at the patched config instead of the model's original.
config-format: "hf"
load-format: "safetensors"
hf-config-path: "/workspace/config_override"

# ---- Tool calling (REQUIRED for Mistral Vibe CLI v2.0.2) ----
# Without these two flags, Mistral Vibe CLI cannot perform any agentic actions
# (file reads, writes, terminal commands). The model generates [TOOL_CALLS] tokens
# that vLLM's MistralToolParser converts into OpenAI-compatible tool_calls objects.
enable-auto-tool-choice: true
tool-call-parser: "mistral"

# ---- Text-only mode ----
# Devstral Small 2 inherits a Pixtral vision encoder from its Mistral Small 3.1 24B
# base model, but Devstral is a text/code-only model — the vision encoder is dead weight.
# Setting image limit to 0 activates vLLM's text-only mode, which skips loading the
# vision tower and multi-modal projector (~1 GiB BF16), freeing that VRAM for KV cache.
limit-mm-per-prompt: '{"image": 0}'

# ---- Memory layout on NVIDIA GeForce RTX 5090 (32 GiB GDDR7) ----
#
# Measured by vLLM v0.15.1 at startup (nvidia-smi reports 30,838 MiB total used):
#
#   Model loading ................. 14.29 GiB  (vLLM log: "Model loading took 14.29 GiB")
#   CUDA driver + runtime ......... ~0.59 GiB  (unavoidable; 31.36 - 30.77 GiB free at init)
#   torch.compile + CUDA graphs ... ~0.55 GiB  (reclaimed after graph capture: "took -0.55 GiB")
#   BF16 KV cache ................. 15.07 GiB  (vLLM log: "Available KV cache memory: 15.07 GiB")
#   ───────────────────────────────────────────
#   Total ......................... ~30.50 GiB  (of 31.36 GiB visible to container)
#
# KV cache stays in full precision (BF16) to preserve attention quality over long
# contexts. It is enough that we already sacrifice quality with quantized weights —
# quantizing the KV cache (FP8) on top of that compounds precision loss in ways that
# hurt the model's ability to attend accurately to context thousands of tokens back.
# Full-precision KV is non-negotiable when context fidelity matters.
#
# 0.98 is the practical ceiling — CUDA driver/runtime consumes ~1.9% of VRAM before
# vLLM starts allocating, so 0.99 fails with "Free memory ... is less than desired".
gpu-memory-utilization: 0.98

# 98,768 tokens — the exact KV cache capacity reported by vLLM at 0.98 utilization.
# The model supports up to 256K via YaRN rope scaling, but with full-precision BF16
# KV cache on 32 GiB VRAM, 98,768 is the absolute ceiling.
#
# KV cache math: 2 (K+V) x 40 layers x 8 KV heads x 128 dim x 2 bytes (BF16)
#              = 163,840 bytes/token = 160 KiB/token
# 98,768 tokens x 160 KiB = ~15.07 GiB
max-model-len: 98768

# Single concurrent request. This dedicates the full KV cache budget to one
# Mistral Vibe CLI session — the intended single-user agentic coding workflow.
max-num-seqs: 1

# ---- Server-side generation defaults ----
# Use "vllm" (not "auto") because the AWQ repo's generation_config.json is
# incomplete — it is missing temperature and do_sample. With "vllm", only
# our explicit overrides below apply, making this file the single source of truth.
#
# CRITICAL: vLLM's default max_tokens is only 16 if not overridden here.
# Without max_new_tokens, the model would silently truncate every response at 16 tokens.
#
# Mistral AI recommends temperature 0.15 for Devstral Small 2 agentic coding tasks
# (from the official generation_config.json at mistralai/Devstral-Small-2-24B-Instruct-2512).
# Note: Mistral's blog post says 0.2, but the generation_config.json (updated Dec 22, 2025
# via https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512/discussions/24)
# is the authoritative source. 0.15 is what ships with the model weights.
generation-config: "vllm"
override-generation-config: >
  {
    "temperature": 0.15,
    "max_new_tokens": 98768
  }
